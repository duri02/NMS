{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V4 — 01 RAG Ingest (Pinecone)\n",
        "Este notebook es la versión **ASCII-safe** para cargar tu RAG a Pinecone.\n",
        "\n",
        "Sugerencia: trabaja desde la carpeta `natubot_v4/` y ajusta `INPUT_PATH` a tus archivos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Natubot — Transform + Load RAG → Pinecone (Gemini Embeddings) — v3 (ASCII-safe IDs)\n",
        "Este notebook:\n",
        "1) Normaliza tu RAG al contrato `rag_contract_v1` (metadata plana, sin null, tipos válidos).\n",
        "2) Genera embeddings con Gemini (`gemini-embedding-001`, 768 dims).\n",
        "3) Hace upsert a Pinecone.\n",
        "4) Incluye celdas extra para **listar/crear/verificar** tu índice de Pinecone y obtener el **host**.\n",
        "\n",
        "## Requisitos\n",
        "- Python 3.9+\n",
        "- `pip install -U google-genai \"pinecone[grpc]\" pydantic pydantic-settings`\n",
        "\n",
        "## Variables de entorno (recomendado)\n",
        "- `GEMINI_API_KEY`\n",
        "- `PINECONE_API_KEY`\n",
        "- `PINECONE_INDEX_NAME` (nombre del índice)\n",
        "- (opcional) `PINECONE_INDEX_HOST` (si ya lo tienes)\n",
        "- (opcional) `PINECONE_NAMESPACE`\n",
        "- Si vas a **crear** índice desde este notebook (opcional):\n",
        "  - `PINECONE_CLOUD` (ej: aws)\n",
        "  - `PINECONE_REGION` (ej: us-east-1)\n",
        "\n",
        "Abre este `.ipynb` en VS Code y ejecútalo celda por celda."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Instalar dependencias\n",
        "Si ya estás en tu venv con dependencias instaladas, puedes saltarte esta celda."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Ejecuta en tu terminal (no dentro del notebook si no quieres):\n",
        "# pip install -U google-genai \"pinecone[grpc]\" pydantic pydantic-settings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Config (aquí pones tus keys / nombre del índice / namespace)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime, timezone\n",
        "import json\n",
        "import hashlib\n",
        "from typing import Any, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "# ===============\n",
        "# INPUTS\n",
        "# ===============\n",
        "# En VS Code, usa rutas locales:\n",
        "INPUT_PATH = \"./rag_documents.jsonl\"        # o \"./all_products_merged.json\"\n",
        "OUTPUT_JSONL_PATH = \"./rag_contract_v1.jsonl\"  # salida normalizada (opcional)\n",
        "\n",
        "# ===============\n",
        "# GEMINI\n",
        "# ===============\n",
        "# Recomendado: usar env vars. Si quieres pegar la key aquí para pruebas, reemplaza el valor.\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\", \"\")\n",
        "EMBED_MODEL = \"gemini-embedding-001\"\n",
        "EMBED_DIM = 768  # recomendado para Pinecone (menos costo/almacenamiento)\n",
        "\n",
        "# ===============\n",
        "# PINECONE\n",
        "# ===============\n",
        "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n",
        "# Nombre del índice (\"database\" en tu lenguaje). Esto es lo más importante.\n",
        "PINECONE_INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\", \"natubot-index\")\n",
        "# Host del índice (si no lo tienes, este notebook puede intentar resolverlo desde el nombre)\n",
        "PINECONE_INDEX_HOST = os.getenv(\"PINECONE_INDEX_HOST\", \"\")\n",
        "\n",
        "# Namespace: partición lógica dentro del índice (útil para separar datasets/versions)\n",
        "PINECONE_NAMESPACE = os.getenv(\"PINECONE_NAMESPACE\", \"natubot\")\n",
        "\n",
        "# Si quieres crear el índice desde el notebook (opcional)\n",
        "PINECONE_CLOUD = os.getenv(\"PINECONE_CLOUD\", \"aws\")\n",
        "PINECONE_REGION = os.getenv(\"PINECONE_REGION\", \"us-east-1\")\n",
        "PINECONE_METRIC = os.getenv(\"PINECONE_METRIC\", \"cosine\")\n",
        "\n",
        "# ===============\n",
        "# Contract / ops\n",
        "# ===============\n",
        "SCHEMA_VERSION = \"rag_contract_v1\"\n",
        "DATA_VERSION = \"mns_2019_v1\"\n",
        "RECORD_TYPE = \"product_info\"\n",
        "CONTENT_STATUS_DEFAULT = \"complete\"\n",
        "\n",
        "# ===============\n",
        "# Batch sizes\n",
        "# ===============\n",
        "EMBED_BATCH_SIZE = 32\n",
        "UPSERT_BATCH_SIZE = 200\n",
        "\n",
        "# ===============\n",
        "# Safety caps\n",
        "# ===============\n",
        "# Pinecone: metadata por record debe estar <= 40KB.\n",
        "MAX_TEXT_CHARS = 12000\n",
        "MAX_METADATA_BYTES = 38000  # margen bajo 40KB\n",
        "\n",
        "print(\"INPUT_PATH:\", INPUT_PATH)\n",
        "print(\"PINECONE_INDEX_NAME:\", PINECONE_INDEX_NAME)\n",
        "print(\"PINECONE_NAMESPACE:\", PINECONE_NAMESPACE)\n",
        "print(\"PINECONE_INDEX_HOST (si ya lo tienes):\", PINECONE_INDEX_HOST or \"(vacío)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Validación rápida de configuración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "missing = []\n",
        "if not GEMINI_API_KEY:\n",
        "    missing.append(\"GEMINI_API_KEY\")\n",
        "if not PINECONE_API_KEY:\n",
        "    missing.append(\"PINECONE_API_KEY\")\n",
        "if not PINECONE_INDEX_NAME:\n",
        "    missing.append(\"PINECONE_INDEX_NAME\")\n",
        "\n",
        "if missing:\n",
        "    print(\"Faltan variables:\", \", \".join(missing))\n",
        "    print(\"Recomendado: setearlas como env vars antes de ejecutar.\")\n",
        "else:\n",
        "    print(\"OK: variables mínimas presentes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Pinecone — listar índices / crear índice / resolver host\n",
        "Si ya tienes el **host**, puedes saltarte a la siguiente celda.\n",
        "\n",
        "Notas:\n",
        "- Este bloque usa el cliente de control-plane (`pinecone`) para listar/crear/describir índices.\n",
        "- El bloque de upsert/query usará el cliente gRPC (`pinecone[grpc]`) y requiere `host`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# 1) Listar índices disponibles\n",
        "try:\n",
        "    idx_list = pc.list_indexes()\n",
        "    # Compatibilidad: a veces devuelve objeto con .names()\n",
        "    names = idx_list.names() if hasattr(idx_list, \"names\") else idx_list\n",
        "    print(\"Índices en tu cuenta:\", names)\n",
        "except Exception as e:\n",
        "    print(\"No se pudo listar índices:\", repr(e))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 2) Crear índice si NO existe (opcional)\n",
        "# Solo corre esta celda si quieres que el notebook cree el índice.\n",
        "# Debe coincidir con EMBED_DIM (768) y métrica cosine.\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "def index_exists(name: str) -> bool:\n",
        "    try:\n",
        "        idx_list = pc.list_indexes()\n",
        "        names = idx_list.names() if hasattr(idx_list, \"names\") else idx_list\n",
        "        return name in list(names)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "if index_exists(PINECONE_INDEX_NAME):\n",
        "    print(\"El índice ya existe:\", PINECONE_INDEX_NAME)\n",
        "else:\n",
        "    print(\"Creando índice:\", PINECONE_INDEX_NAME)\n",
        "    pc.create_index(\n",
        "        name=PINECONE_INDEX_NAME,\n",
        "        dimension=EMBED_DIM,\n",
        "        metric=PINECONE_METRIC,\n",
        "        spec=ServerlessSpec(cloud=PINECONE_CLOUD, region=PINECONE_REGION),\n",
        "    )\n",
        "    print(\"Solicitud de creación enviada. (Puede tardar un poco en estar listo)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# 3) Describir índice y resolver HOST (si PINECONE_INDEX_HOST está vacío)\n",
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "desc = pc.describe_index(PINECONE_INDEX_NAME)\n",
        "# Compatibilidad: desc puede ser dict u objeto\n",
        "host = None\n",
        "if isinstance(desc, dict):\n",
        "    host = desc.get(\"host\") or desc.get(\"status\", {}).get(\"host\")\n",
        "else:\n",
        "    host = getattr(desc, \"host\", None) or getattr(getattr(desc, \"status\", None), \"host\", None)\n",
        "\n",
        "if host:\n",
        "    print(\"HOST del índice:\", host)\n",
        "    if not PINECONE_INDEX_HOST:\n",
        "        PINECONE_INDEX_HOST = host\n",
        "        print(\"PINECONE_INDEX_HOST actualizado en memoria (solo para esta sesión del notebook).\")\n",
        "else:\n",
        "    print(\"No pude resolver el host. Revisa en la consola de Pinecone y pega PINECONE_INDEX_HOST manualmente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Helpers: lectura, normalización de metadata (Pinecone), hashing e IDs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import hashlib\n",
        "import re\n",
        "import unicodedata\n",
        "from datetime import datetime, timezone\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "def to_ascii_id(s: str, max_len: int = 512) -> str:\n",
        "    \"\"\"Convierte a ASCII seguro para IDs de Pinecone:\n",
        "    - quita acentos/diacríticos (Caléndula -> Calendula)\n",
        "    - reemplaza caracteres no permitidos por '_'\n",
        "    \"\"\"\n",
        "    s = str(s or \"\")\n",
        "    s = unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "    s = s.strip()\n",
        "    s = re.sub(r\"[^A-Za-z0-9._:-]+\", \"_\", s)\n",
        "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
        "    return (s[:max_len] if s else \"unknown\")\n",
        "\n",
        "def _sha256(text: str) -> str:\n",
        "    return hashlib.sha256(text.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "def _utc_iso() -> str:\n",
        "    return datetime.now(timezone.utc).isoformat()\n",
        "\n",
        "def _is_flat_value(v: Any) -> bool:\n",
        "    # Pinecone metadata values válidos: string/number/bool/list[str]; no dict; no null.\n",
        "    if v is None:\n",
        "        return False\n",
        "    if isinstance(v, (str, int, float, bool)):\n",
        "        return True\n",
        "    if isinstance(v, list) and all(isinstance(x, str) for x in v):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _to_list_str(v: Any) -> Optional[List[str]]:\n",
        "    if v is None:\n",
        "        return None\n",
        "    if isinstance(v, list):\n",
        "        out = []\n",
        "        for x in v:\n",
        "            if x is None:\n",
        "                continue\n",
        "            s = str(x).strip()\n",
        "            if s:\n",
        "                out.append(s)\n",
        "        return out or None\n",
        "    if isinstance(v, str):\n",
        "        parts = [p.strip() for p in v.split(\",\")]\n",
        "        parts = [p for p in parts if p]\n",
        "        return parts or None\n",
        "    s = str(v).strip()\n",
        "    return [s] if s else None\n",
        "\n",
        "def _clean_metadata(md: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Enforce Pinecone constraints:\n",
        "      - flat JSON, no nested objects\n",
        "      - no null values (omit keys)\n",
        "      - values only: str/number/bool/list[str]\n",
        "      - keys must not start with '$'\n",
        "    \"\"\"\n",
        "    clean: Dict[str, Any] = {}\n",
        "    for k, v in (md or {}).items():\n",
        "        if v is None:\n",
        "            continue\n",
        "        k = str(k)\n",
        "        if not k or k.startswith(\"$\"):\n",
        "            continue\n",
        "\n",
        "        # Normalize lists\n",
        "        if k in {\"health_goal_tags\", \"ingredient_tags\", \"source_pages\"}:\n",
        "            v = _to_list_str(v)\n",
        "            if not v:\n",
        "                continue\n",
        "\n",
        "        # Pinecone metadata no soporta dicts\n",
        "        if isinstance(v, dict):\n",
        "            continue\n",
        "\n",
        "        # If list but not list[str] -> coerce\n",
        "        if isinstance(v, list) and not all(isinstance(x, str) for x in v):\n",
        "            v = _to_list_str(v)\n",
        "            if not v:\n",
        "                continue\n",
        "\n",
        "        # Final type check\n",
        "        if not _is_flat_value(v):\n",
        "            continue\n",
        "\n",
        "        # Drop empty strings / empty lists\n",
        "        if isinstance(v, str) and not v.strip():\n",
        "            continue\n",
        "        if isinstance(v, list) and len(v) == 0:\n",
        "            continue\n",
        "\n",
        "        clean[k] = v\n",
        "\n",
        "    return clean\n",
        "\n",
        "def _metadata_bytes(md: Dict[str, Any]) -> int:\n",
        "    return len(json.dumps(md, ensure_ascii=False).encode(\"utf-8\"))\n",
        "\n",
        "def _truncate_text_to_fit(md: Dict[str, Any], text_key: str = \"text\") -> Dict[str, Any]:\n",
        "    \"\"\"Garantiza que metadata <= MAX_METADATA_BYTES truncando `text` si hace falta.\"\"\"\n",
        "    md = dict(md)\n",
        "    text = md.get(text_key, \"\") or \"\"\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # cap básico\n",
        "    if len(text) > MAX_TEXT_CHARS:\n",
        "        text = text[:MAX_TEXT_CHARS].rstrip()\n",
        "        md[text_key] = text\n",
        "\n",
        "    # cap por bytes\n",
        "    while _metadata_bytes(md) > MAX_METADATA_BYTES and len(text) > 2000:\n",
        "        text = text[: int(len(text) * 0.9)].rstrip()\n",
        "        md[text_key] = text\n",
        "\n",
        "    return md\n",
        "\n",
        "def _build_chunk_id(product_id: str, section: str, chunk_index: int, content_hash: str) -> str:\n",
        "    # Pinecone exige Vector ID ASCII. Sanitizamos product_id/section.\n",
        "    pid = to_ascii_id(product_id)\n",
        "    sec = to_ascii_id(section)\n",
        "    return f\"{pid}::{sec}::{int(chunk_index)}::{content_hash[:8]}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Cargar input (JSONL chunks o JSON productos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def load_json(path: str) -> Any:\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "raw = None\n",
        "if INPUT_PATH.lower().endswith(\".jsonl\"):\n",
        "    raw = load_jsonl(INPUT_PATH)\n",
        "    input_mode = \"jsonl_chunks\"\n",
        "elif INPUT_PATH.lower().endswith(\".json\"):\n",
        "    raw = load_json(INPUT_PATH)\n",
        "    input_mode = \"json_products\"\n",
        "else:\n",
        "    raise ValueError(\"INPUT_PATH debe ser .jsonl o .json\")\n",
        "\n",
        "print(\"input_mode =\", input_mode)\n",
        "print(\"records =\", len(raw) if isinstance(raw, list) else type(raw))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Transformar a `rag_contract_v1`\n",
        "Ajusta `SECTION_FIELDS` si tu JSON de productos usa llaves distintas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "PRODUCT_ID_FIELD = \"product_id\"\n",
        "PRODUCT_NAME_FIELD = \"product_name\"\n",
        "\n",
        "SECTION_FIELDS = {\n",
        "    \"overview\": [\"description\", \"benefits\", \"indications\", \"how_it_works\"],\n",
        "    \"ingredients\": [\"ingredients\", \"active_ingredients\", \"composition\"],\n",
        "    \"usage_safety\": [\"dosage\", \"usage\", \"warnings\", \"contraindications\", \"storage\"],\n",
        "}\n",
        "\n",
        "def build_section_text(product: Dict[str, Any], section: str) -> str:\n",
        "    parts = []\n",
        "    for key in SECTION_FIELDS.get(section, []):\n",
        "        val = product.get(key)\n",
        "        if val is None:\n",
        "            continue\n",
        "        if isinstance(val, list):\n",
        "            val = \", \".join([str(x) for x in val if x is not None])\n",
        "        val = str(val).strip()\n",
        "        if not val:\n",
        "            continue\n",
        "        parts.append(f\"{key}: {val}\")\n",
        "    return \"\\n\".join(parts).strip()\n",
        "\n",
        "def normalize_chunks_from_products(products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    chunks: List[Dict[str, Any]] = []\n",
        "    for p in products:\n",
        "        product_id = str(p.get(PRODUCT_ID_FIELD, \"\")).strip() or \"unknown_product\"\n",
        "        product_name = str(p.get(PRODUCT_NAME_FIELD, \"\")).strip() or \"unknown_product_name\"\n",
        "\n",
        "        section_texts: List[Tuple[str, str]] = []\n",
        "        for section in SECTION_FIELDS.keys():\n",
        "            txt = build_section_text(p, section)\n",
        "            if txt:\n",
        "                section_texts.append((section, txt))\n",
        "\n",
        "        chunk_total = max(1, len(section_texts))\n",
        "        if not section_texts:\n",
        "            section_texts = [(\"overview\", json.dumps(p, ensure_ascii=False)[:MAX_TEXT_CHARS])]\n",
        "\n",
        "        for idx, (section, text) in enumerate(section_texts):\n",
        "            text = text[:MAX_TEXT_CHARS].strip()\n",
        "            content_hash = _sha256(text)\n",
        "\n",
        "            md = {\n",
        "                \"text\": text,\n",
        "                \"product_id\": product_id,\n",
        "                \"product_name\": product_name,\n",
        "                \"section\": section,\n",
        "                \"language\": p.get(\"language\", \"es\"),\n",
        "                \"health_goal_tags\": p.get(\"health_goal_tags\"),\n",
        "                \"ingredient_tags\": p.get(\"ingredient_tags\"),\n",
        "                \"product_type\": p.get(\"product_type\"),\n",
        "                \"dosage_form\": p.get(\"dosage_form\"),\n",
        "                \"registration_number\": p.get(\"registration_number\"),\n",
        "                \"source_pdf\": p.get(\"source_pdf\"),\n",
        "                \"source_pages\": p.get(\"source_pages\"),\n",
        "                \"content_status\": p.get(\"content_status\", CONTENT_STATUS_DEFAULT),\n",
        "                \"record_type\": RECORD_TYPE,\n",
        "                \"content_hash\": content_hash,\n",
        "                \"chunk_index\": idx,\n",
        "                \"chunk_total\": chunk_total,\n",
        "                \"data_version\": DATA_VERSION,\n",
        "                \"schema_version\": SCHEMA_VERSION,\n",
        "                \"ingested_at\": _utc_iso(),\n",
        "            }\n",
        "\n",
        "            md = _clean_metadata(md)\n",
        "            md = _truncate_text_to_fit(md, \"text\")\n",
        "            chunk_id = _build_chunk_id(product_id, section, idx, md[\"content_hash\"])\n",
        "            chunks.append({\"id\": chunk_id, \"text\": md[\"text\"], \"metadata\": md})\n",
        "    return chunks\n",
        "\n",
        "def normalize_chunks_from_jsonl(rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    chunks: List[Dict[str, Any]] = []\n",
        "    for r in rows:\n",
        "        md = r.get(\"metadata\", {}) if isinstance(r, dict) else {}\n",
        "        text = r.get(\"text\") or md.get(\"text\") or \"\"\n",
        "        text = str(text).strip()\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        product_id = str(md.get(\"product_id\", r.get(\"product_id\", \"unknown_product\"))).strip()\n",
        "        product_name = str(md.get(\"product_name\", r.get(\"product_name\", \"unknown_product_name\"))).strip()\n",
        "        section = str(md.get(\"section\", r.get(\"section\", \"overview\"))).strip() or \"overview\"\n",
        "        language = str(md.get(\"language\", r.get(\"language\", \"es\"))).strip() or \"es\"\n",
        "\n",
        "        text = text[:MAX_TEXT_CHARS].strip()\n",
        "        content_hash = _sha256(text)\n",
        "\n",
        "        md_final = dict(md)\n",
        "        md_final.update({\n",
        "            \"text\": text,\n",
        "            \"product_id\": product_id,\n",
        "            \"product_name\": product_name,\n",
        "            \"section\": section,\n",
        "            \"language\": language,\n",
        "            \"record_type\": md.get(\"record_type\", RECORD_TYPE),\n",
        "            \"content_hash\": md.get(\"content_hash\", content_hash),\n",
        "            \"schema_version\": md.get(\"schema_version\", SCHEMA_VERSION),\n",
        "            \"data_version\": md.get(\"data_version\", DATA_VERSION),\n",
        "            \"ingested_at\": md.get(\"ingested_at\", _utc_iso()),\n",
        "        })\n",
        "\n",
        "        md_final.setdefault(\"content_status\", CONTENT_STATUS_DEFAULT)\n",
        "        md_final.setdefault(\"chunk_index\", int(md_final.get(\"chunk_index\", 0) or 0))\n",
        "        md_final.setdefault(\"chunk_total\", int(md_final.get(\"chunk_total\", 1) or 1))\n",
        "\n",
        "        md_final = _clean_metadata(md_final)\n",
        "        md_final = _truncate_text_to_fit(md_final, \"text\")\n",
        "\n",
        "        chunk_id = r.get(\"id\")\n",
        "        if chunk_id:\n",
        "            chunk_id = to_ascii_id(chunk_id)\n",
        "        else:\n",
        "            chunk_id = _build_chunk_id(product_id, section, md_final[\"chunk_index\"], md_final[\"content_hash\"])\n",
        "        chunks.append({\"id\": chunk_id, \"text\": md_final[\"text\"], \"metadata\": md_final})\n",
        "\n",
        "    return chunks\n",
        "\n",
        "if input_mode == \"json_products\":\n",
        "    chunks = normalize_chunks_from_products(raw)\n",
        "else:\n",
        "    chunks = normalize_chunks_from_jsonl(raw)\n",
        "\n",
        "print(\"normalized chunks:\", len(chunks))\n",
        "print(\"sample:\", json.dumps(chunks[0], ensure_ascii=False)[:900], \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) (Opcional) Guardar JSONL normalizado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "with open(OUTPUT_JSONL_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for c in chunks:\n",
        "        f.write(json.dumps(c, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Wrote:\", OUTPUT_JSONL_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Embeddings con Gemini (batch) + Upsert a Pinecone (gRPC)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from pinecone.grpc import PineconeGRPC as PineconeGRPC\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"Falta GEMINI_API_KEY\")\n",
        "if not PINECONE_API_KEY:\n",
        "    raise ValueError(\"Falta PINECONE_API_KEY\")\n",
        "if not PINECONE_INDEX_HOST:\n",
        "    raise ValueError(\"Falta PINECONE_INDEX_HOST. Ejecuta la celda para resolver host o pégalo manualmente.\")\n",
        "\n",
        "gclient = genai.Client(api_key=GEMINI_API_KEY)\n",
        "\n",
        "pc_grpc = PineconeGRPC(api_key=PINECONE_API_KEY)\n",
        "index = pc_grpc.Index(host=PINECONE_INDEX_HOST)\n",
        "\n",
        "def batched(lst: List[Any], n: int) -> Iterable[List[Any]]:\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i+n]\n",
        "\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    res = gclient.models.embed_content(\n",
        "        model=EMBED_MODEL,\n",
        "        contents=texts,\n",
        "        config=types.EmbedContentConfig(\n",
        "            task_type=\"RETRIEVAL_DOCUMENT\",\n",
        "            output_dimensionality=EMBED_DIM,\n",
        "        ),\n",
        "    )\n",
        "    return [e.values for e in res.embeddings]\n",
        "\n",
        "upserted = 0\n",
        "\n",
        "for batch_docs in batched(chunks, EMBED_BATCH_SIZE):\n",
        "    texts = [d[\"metadata\"][\"text\"] for d in batch_docs]\n",
        "    vectors = embed_texts(texts)\n",
        "\n",
        "    pinecone_vectors = []\n",
        "    for d, v in zip(batch_docs, vectors):\n",
        "        pinecone_vectors.append({\n",
        "            \"id\": d[\"id\"],\n",
        "            \"values\": v,\n",
        "            \"metadata\": d[\"metadata\"],\n",
        "        })\n",
        "\n",
        "    index.upsert(vectors=pinecone_vectors, namespace=PINECONE_NAMESPACE)\n",
        "    upserted += len(pinecone_vectors)\n",
        "\n",
        "    if upserted % (EMBED_BATCH_SIZE * 10) == 0:\n",
        "        print(\"Upserted so far:\", upserted)\n",
        "\n",
        "print(\"DONE. Total upserted:\", upserted)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Ver stats del índice (namespaces, conteos)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "stats = index.describe_index_stats(namespace=PINECONE_NAMESPACE)\n",
        "print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Smoke test: query top_k (Gemini RETRIEVAL_QUERY + Pinecone query)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def embed_query(text: str) -> List[float]:\n",
        "    res = gclient.models.embed_content(\n",
        "        model=EMBED_MODEL,\n",
        "        contents=text,\n",
        "        config=types.EmbedContentConfig(\n",
        "            task_type=\"RETRIEVAL_QUERY\",\n",
        "            output_dimensionality=EMBED_DIM,\n",
        "        ),\n",
        "    )\n",
        "    return res.embeddings[0].values\n",
        "\n",
        "query = \"¿Para qué sirve este producto y cómo se usa?\"\n",
        "qvec = embed_query(query)\n",
        "\n",
        "res = index.query(\n",
        "    namespace=PINECONE_NAMESPACE,\n",
        "    vector=qvec,\n",
        "    top_k=5,\n",
        "    include_metadata=True,\n",
        "    include_values=False,\n",
        ")\n",
        "\n",
        "matches = getattr(res, \"matches\", []) or []\n",
        "print(\"matches:\", len(matches))\n",
        "\n",
        "for i, m in enumerate(matches, start=1):\n",
        "    md = m.metadata or {}\n",
        "    print(f\"\\n#{i} score={m.score:.4f} id={m.id}\")\n",
        "    print(\"product:\", md.get(\"product_name\"), \"| section:\", md.get(\"section\"))\n",
        "    snippet = md.get(\"text\",\"\")\n",
        "    print(\"text_snippet:\", (snippet[:220] + \"...\") if snippet else \"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}